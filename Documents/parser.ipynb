{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este reporte se estarán analizando y desarrollando las diferentes partes del transpilador del lenguaje RoadToCivilization al lenguaje python, entre ellas están, el lexer, la gramática, el parser, el análisis semántico y el transpilado a python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "\n",
    "Nuestro lenguaje está formado por conjunto de expresiones regulares cada una asociada a un tipo de token, la unión de estas expresiones regulares sería el conjunto de tokens que conforman nuestro lenguaje. El lexer se encarga de verificar que todos los tokens que nos entren pertenezcan al lenguaje. \n",
    "\n",
    "Para la realización del lexer se crearon todas las expresiones regulares que conforman el lenguaje, seguido a esto se verifica,haciendo match, que cada token pertenezca al conjunto de expresiones regulares ya definidos. Cada expresión regular tiene un nombre para identificar que tipo de token es, luego se devuelven el token y su tipo si no hubo ningún problema al hacer match.\n",
    "\n",
    "Aquí también se verificó que los strings sean correctos, por ejemplo, si empiezan a escribir un string y hacen un salto de línea sería incorrecto y el lexer devuelve error de sintaxis.\n",
    "\n",
    "Nosotros implementamos dos lexer, uno usando la librería re y otro sin usar la misma, ambos funcionan correctamente, pero el que utilizamos es el que usa la librería re, puesto que es un poquito más rápido y el código es más legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "'''\n",
    "Recibe como parametros el lexema, tipo y linea donde se encuentra\n",
    "'''\n",
    "class Token:\n",
    " \n",
    "    def __init__(self, lex, ttype, line): \n",
    "        \n",
    "        self.lex = lex \n",
    "        self.token_type = ttype\n",
    "        self.line = line\n",
    " \n",
    "    def __str__(self): \n",
    "        return f'{self.ttype}: {self.lex}'\n",
    " \n",
    "'''\n",
    "El lexer es el resultado de la union de todas las expresiones regulares que forman\n",
    "el lenguaje\n",
    "'''\n",
    "class Lexer: \n",
    "\n",
    "    def __init__(self, table, keywords, ignored_tokens, eof):\n",
    "        self.line = 1\n",
    "        self.table = table\n",
    "        self.keywords = keywords\n",
    "        self.ignored_tokens = ignored_tokens\n",
    "        self.regex = self._build_regex(table)\n",
    "        self.errors = []\n",
    "        self.eof = eof\n",
    "        \n",
    " \n",
    "    def tokenize(self,text):    \n",
    "        while len(text) > 0:\n",
    "\n",
    "            match = self.regex.match(text)\n",
    "            error_token = ''\n",
    " \n",
    "            while not match:\n",
    "                error_token += text[0]\n",
    "                text = text[1:]\n",
    "                if len(text) <= 0: break\n",
    "                match = self.regex.match(text)\n",
    "\n",
    "            if error_token:\n",
    "                self.errors.append(f'Syntax error, unexpexted token \"{error_token}\" at line {self.line}')\n",
    "                if len(text) <= 0: continue\n",
    "\n",
    "            lexeme = match.group()\n",
    "\n",
    "            if lexeme == '\\n':\n",
    "                self.line += 1\n",
    "\n",
    "\n",
    "            # STRINGS\n",
    "            elif lexeme == '\"':\n",
    "                text = text[1:]\n",
    "                while len(text) > 0:\n",
    "                    c = text[0]\n",
    "                    text = text[1:]\n",
    "\n",
    "                    if c == '\\\\':\n",
    "                        if text[0] == 'b':\n",
    "                            lexeme += '\\\\b'\n",
    "\n",
    "                        elif text[0] == 't':\n",
    "                            lexeme += '\\\\t'\n",
    "\n",
    "                        elif text[0] == 'n':\n",
    "                            lexeme += '\\\\n'\n",
    "\n",
    "                        elif text[0] == 'f':\n",
    "                            lexeme += '\\\\f'\n",
    "                        \n",
    "                        else:\n",
    "                            lexeme += text[0]\n",
    "\n",
    "                        text = text[1:]\n",
    "                    \n",
    "                    elif c == '\\n':\n",
    "                        self.errors.append(f'Syntax error at line {self.line} : Undefined string')\n",
    "                        self.line += 1\n",
    "                        break\n",
    "                    \n",
    "                    elif c == '\\0':\n",
    "                        self.errors.append(f'Syntax error at line {self.line} : String cannot contain the null character')\n",
    "                    \n",
    "                    else:\n",
    "                        lexeme += c\n",
    "                        if c == '\"':\n",
    "                            break\n",
    "                \n",
    "                else:\n",
    "                    self.errors.append(f'Syntax error at line {self.line} : String cannot contain EOF')\n",
    "\n",
    "\n",
    "\n",
    "            token_type = match.lastgroup if lexeme.lower() not in self.keywords and match.lastgroup is not None else match.group().lower()\n",
    " \n",
    "            yield lexeme, token_type, self.line\n",
    " \n",
    "            text = text[match.end():] if lexeme[0] != '\"' else text\n",
    "\n",
    "        yield '$', self.eof, self.line\n",
    " \n",
    "    def _build_regex(sef,table):\n",
    "        return re.compile('|'.join([f'(?P<{name}>{regex})' if name != regex else f'({name})' for name,regex in table.items()]))\n",
    " \n",
    "    def __call__(self, text): \n",
    "        return [Token(lex, ttype, line) for lex, ttype, line in self.tokenize(text) if ttype not in self.ignored_tokens]\n",
    "\n",
    "'''\n",
    "Esta clase guardara las propiedades sintacticas del lenguaje\n",
    "'''\n",
    "class MyLexer(Lexer):\n",
    "    def __init__(self):\n",
    "        self.regexs = {\n",
    "        'var'       : r'[a-z][a-zA-Z0-9_]*'         ,\n",
    "        'type'      : r'[A-Z][a-zA-Z0-9_]*'         ,\n",
    "        'funct_name': r'_[a-zA-Z0-9_]*'             ,\n",
    "        'string'    : r'\\\"'                         ,\n",
    "        'number'    : r'(\\(-\\d+(\\.\\d+)?\\))|(\\d+(\\.\\d+)?)',\n",
    "        'newline'   : r'\\n'                         ,\n",
    "        'whitespace': r' +'                         ,\n",
    "        'tabulation': r'\\t+'                        ,\n",
    "        'while'     : r'while'                      , \n",
    "        'false'     : r'false'                      , \n",
    "        'else'      : r'else'                       , \n",
    "        'true'      : r'true'                       ,\n",
    "        'not'       : r'not'                        ,\n",
    "        'or'        : r'or'                         , \n",
    "        'and'       : r'and'                        ,\n",
    "        '\\{'        : r'\\{'                         ,\n",
    "        '\\}'        : r'\\}'                         ,\n",
    "        '\\('        : r'\\('                         ,\n",
    "        '\\)'        : r'\\)'                         ,\n",
    "        '\\['        : r'\\['                         ,\n",
    "        '\\]'        : r'\\]'                         ,\n",
    "        '\\.'        : r'\\.'                         , \n",
    "        'if'        : r'if'                         ,  \n",
    "        '\\+'        : r'\\+'                         , \n",
    "        '\\-'        : r'\\-'                         , \n",
    "        '\\*'        : r'\\*'                         ,\n",
    "        ','         : r','                          ,\n",
    "        ';'         : r';'                          ,\n",
    "        '/'         : r'/'                          , \n",
    "        '<'         : r'<'                          , \n",
    "        '>'         : r'>'                          , \n",
    "        '='         : r'='                          ,\n",
    "        '=='        : r'=='                         }\n",
    "               \n",
    "                   \n",
    "        self.keywords = ['while','false', 'else', 'not','if','and','or', 'true']\n",
    "        \n",
    "        self.ignored_tokens = ['newline','whitespace','tabulation']\n",
    "\n",
    "        Lexer.__init__(self, self.regexs, self.keywords, self.ignored_tokens, 'eof')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gramática\n",
    "\n",
    "El diseño de la gramática fue algo bastante trabajoso, fue una de las partes más difíciles, para modelarla se usó la clase Grammar que se encuentra en en la carpeta cmp."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
