{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este reporte se estarán analizando y desarrollando las diferentes partes del transpilador del lenguaje RoadToCivilization al lenguaje python, entre ellas están, el lexer, la gramática, el parser, el análisis semántico y el transpilado a python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "\n",
    "Nuestro lenguaje está formado por conjunto de expresiones regulares cada una asociada a un tipo de token, la unión de estas expresiones regulares sería el conjunto de tokens que conforman nuestro lenguaje. El lexer se encarga de verificar que todos los tokens que nos entren pertenezcan al lenguaje. \n",
    "\n",
    "Para la realización del lexer se crearon todas las expresiones regulares que conforman el lenguaje, seguido a esto se verifica,haciendo match, que cada token pertenezca al conjunto de expresiones regulares ya definidos. Cada expresión regular tiene un nombre para identificar que tipo de token es, luego se devuelven el token y su tipo si no hubo ningún problema al hacer match.\n",
    "\n",
    "Aquí también se verificó que los strings sean correctos, por ejemplo, si empiezan a escribir un string y hacen un salto de línea sería incorrecto y el lexer devuelve error de sintaxis.\n",
    "\n",
    "Nosotros implementamos dos lexer, uno usando la librería re y otro sin usar la misma, ambos funcionan correctamente, pero el que utilizamos es el que usa la librería re, puesto que es un poquito más rápido y el código es más legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "path.append(os.path.abspath(os.path.join('',os.pardir)))\n",
    "\n",
    "def tokenize(self,text):    \n",
    "    while len(text) > 0:\n",
    "\n",
    "        match = self.regex.match(text)\n",
    "        error_token = ''\n",
    " \n",
    "        while not match:\n",
    "            error_token += text[0]\n",
    "            text = text[1:]\n",
    "            if len(text) <= 0: break\n",
    "            match = self.regex.match(text)\n",
    "\n",
    "        if error_token:\n",
    "            self.errors.append(f'Syntax error, unexpexted token \"{error_token}\" at line {self.line}')\n",
    "            if len(text) <= 0: continue\n",
    "\n",
    "        lexeme = match.group()\n",
    "\n",
    "        if lexeme == '\\n':\n",
    "            self.line += 1\n",
    "\n",
    "        # STRINGS\n",
    "        elif lexeme == '\"':\n",
    "            text = text[1:]\n",
    "            while len(text) > 0:\n",
    "                c = text[0]\n",
    "                text = text[1:]\n",
    "\n",
    "                if c == '\\\\':\n",
    "                    if text[0] == 'b':\n",
    "                        lexeme += '\\\\b'\n",
    "\n",
    "                    elif text[0] == 't':\n",
    "                        lexeme += '\\\\t'\n",
    "\n",
    "                    elif text[0] == 'n':\n",
    "                        lexeme += '\\\\n'\n",
    "\n",
    "                    elif text[0] == 'f':\n",
    "                        lexeme += '\\\\f'\n",
    "                        \n",
    "                    else:\n",
    "                        lexeme += text[0]\n",
    "\n",
    "                    text = text[1:]\n",
    "                    \n",
    "                elif c == '\\n':\n",
    "                    self.errors.append(f'Syntax error at line {self.line} : Undefined string')\n",
    "                    self.line += 1\n",
    "                    break\n",
    "                    \n",
    "                elif c == '\\0':\n",
    "                    self.errors.append(f'Syntax error at line {self.line} : String cannot contain the null character')\n",
    "                    \n",
    "                else:\n",
    "                    lexeme += c\n",
    "                    if c == '\"':\n",
    "                        break    \n",
    "            else:\n",
    "                self.errors.append(f'Syntax error at line {self.line} : String cannot contain EOF')\n",
    "\n",
    "        token_type = match.lastgroup if lexeme.lower() not in self.keywords and match.lastgroup is not None else match.group().lower()\n",
    " \n",
    "        yield lexeme, token_type, self.line\n",
    " \n",
    "        text = text[match.end():] if lexeme[0] != '\"' else text\n",
    "\n",
    "    yield '$', self.eof, self.line\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST\n",
    "\n",
    "Para la construcción del AST se hizo una jerarquía de clases que se compone de la siguiente forma.\n",
    "\n",
    "Primeramente, tenemos los nodos DeclarationNode y ExpressionNode. \n",
    "\n",
    "De DeclarationNode heredan:\n",
    "* DeclarationEntity\n",
    "* DeclarationVar\n",
    "* FuncDeclaration\n",
    "\n",
    "De ExpressionNode heredan:\n",
    "* AsignationVar\n",
    "* FactorNode\n",
    "    * BooleanNode \n",
    "    * StringNode\n",
    "    * NumberNode\n",
    "    * VariableNode\n",
    "    * FunctionName\n",
    "* ListNode\n",
    "* IndexListNode\n",
    "* IfElse\n",
    "* Not\n",
    "* InstanceFunction\n",
    "* WhileNode\n",
    "* BinaryNode\n",
    "    * And\n",
    "    * Or\n",
    "    * LessThan\n",
    "    * MoreThan\n",
    "    * EqualEqual\n",
    "    * PlusNode\n",
    "    * MinusNode\n",
    "    * StarNode\n",
    "    * DivNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ast_nodes:\n",
    "\n",
    "    class Node:\n",
    "        pass\n",
    "\n",
    "\n",
    "    class ProgramNode(Node):\n",
    "        def __init__(self, declarations):\n",
    "            self.declarations = declarations\n",
    "\n",
    "\n",
    "    class DeclarationNode(Node):\n",
    "        pass\n",
    "    class ExpressionNode(Node):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gramática\n",
    "\n",
    "El diseño de la gramática fue algo bastante trabajoso, fue una de las partes más difíciles, para modelarla se usó la clase Grammar que se encuentra en en la carpeta cmp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente definimos los terminales y no terminales, a través de los métodos Terminal y Terminals, y NonTerminal y NonTerminals respectivamente. Con startSymbol accedemos al símbolo distinguido e indicamos cuál es. Luego definimos las producciones haciendo uso del operador %=, con G.Epsilon accederemos a epsilon como su nombre lo indica. El símbolo de fin de cadena se modelará con la clase EOF, que no debe ser instanciada directamente con el constructor, una instancia G de Grammar lo construirá automáticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También, en la gramática, junto con las producciones se construirán los nodos del AST que denota a cada operación, haciendo uso de funciones lambdas que devuleven un nodo o una lista de nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Grammar\n",
    "from Compilacion.astree.AST_Nodes import ast_nodes\n",
    "\n",
    "def build_grammar():\n",
    "    Gram = Grammar()\n",
    "\n",
    "    #No Terminales\n",
    "    program = Gram.NonTerminal('<program>', startSymbol=True)\n",
    "    simulation = Gram.NonTerminal('<simulation>')\n",
    "    declarationnt, arith_expr, funct, term = Gram.NonTerminals('<declarationnt> <arith_expr> <funct> <term>')\n",
    "    crudcharnt, ifnt, cond, whilent = Gram.NonTerminals('<crudcharnt> <ifnt> <cond> <whilent>')\n",
    "    asignationnt, arg_list, type_var = Gram.NonTerminals('<asignationnt> <arg_list> <type_var>')\n",
    "    arg_types_list, functnt, factor = Gram.NonTerminals('<arg_types_list> <functnt> <factor>')\n",
    "    elseblock, listnt, block, listindexed = Gram.NonTerminals('<elseblock> <listnt> <block> <listindexed>')\n",
    "\n",
    "    #Terminales\n",
    "    open_parenthesis, closed_parenthesis, equal, colon, plus, minus, star, div = Gram.Terminals('( ) = , + - * /')\n",
    "    semicolon, point, open_bracket, closed_bracket, open_square, closed_square = Gram.Terminals('; . { } [ ]')\n",
    "    ift, elset, whilet, true, false, string, number = Gram.Terminals('if else while true false string number') \n",
    "    entity, var, funct_name, typet = Gram.Terminals('entity var funct_name type')\n",
    "    arg, lessthan, morethan, equalequal, andt, ort, nott = Gram.Terminals('arg < > == and or not')\n",
    "    \n",
    "    \n",
    "\n",
    "    #Producciones\n",
    "    program %=  simulation, lambda h,s: ast_nodes.ProgramNode(s[1])\n",
    "\n",
    "    simulation %= declarationnt + semicolon, lambda h,s: [s[1]]\n",
    "    simulation %= declarationnt + semicolon + simulation, lambda h,s: [s[1]] + s[3]\n",
    "    simulation %= asignationnt + semicolon, lambda h,s: [s[1]]\n",
    "    simulation %= asignationnt + semicolon + simulation, lambda h,s: [s[1]] + s[3]\n",
    "    simulation %= ifnt, lambda h,s: [s[1]]\n",
    "    simulation %= ifnt + simulation, lambda h,s: [s[1]] + s[2]\n",
    "    simulation %= whilent, lambda h,s: [s[1]]\n",
    "    simulation %= whilent + simulation, lambda h,s: [s[1]] + s[2]\n",
    "    simulation %= functnt, lambda h,s: [s[1]]\n",
    "    simulation %= functnt + simulation, lambda h,s: [s[1]] + s[2]\n",
    "    simulation %= crudcharnt + semicolon, lambda h,s: [s[1]]\n",
    "    simulation %= crudcharnt + semicolon + simulation, lambda h,s: [s[1]] + s[3]\n",
    "    simulation %= funct + semicolon, lambda h,s: [s[1]]\n",
    "    simulation %= funct + semicolon + simulation, lambda h,s: [s[1]] + s[3]\n",
    "\n",
    "    block %= declarationnt + semicolon + block, lambda h,s: [s[1]] + s[3]\n",
    "    block %= asignationnt + semicolon + block, lambda h,s: [s[1]] + s[3]\n",
    "    block %= ifnt + block, lambda h,s: [s[1]] + s[2]\n",
    "    block %= whilent + block, lambda h,s: [s[1]] + s[2]\n",
    "    block %= crudcharnt + semicolon + block, lambda h,s: [s[1]] + s[3]\n",
    "    block %= factor + semicolon + block, lambda h,s: [s[1]] + s[3]\n",
    "    block %= Gram.Epsilon, lambda h,s: []\n",
    "\n",
    "    arg_list %= cond, lambda h,s: [s[1]]\n",
    "    arg_list %= arg_list + colon + cond, lambda h,s: s[1] + [s[3]]\n",
    "    arg_list %= Gram.Epsilon, lambda h,s:  []\n",
    "\n",
    "\n",
    "    declarationnt %= typet + var + equal + typet + open_parenthesis + arg_list + closed_parenthesis, lambda h,s: ast_nodes.DeclarationEntity(s[1],s[2],s[4],s[6])\n",
    "    declarationnt %= typet + var + equal + arith_expr, lambda h,s: ast_nodes.DeclarationVar(s[1],s[2],s[4])  \n",
    "    \n",
    "    asignationnt %= var + equal + arith_expr, lambda h,s: ast_nodes.AsignationVar(s[1],s[3])  \n",
    "\n",
    "    arith_expr %= arith_expr + plus + term, lambda h,s: ast_nodes.PlusNode(s[1],s[3])\n",
    "    arith_expr %= arith_expr + minus + term, lambda h,s: ast_nodes.MinusNode(s[1],s[3])\n",
    "    arith_expr %= term, lambda h,s:s[1]\n",
    "\n",
    "    term %= term + star + factor, lambda h,s: ast_nodes.StarNode(s[1],s[3])\n",
    "    term %= term + div + factor, lambda h,s: ast_nodes.DivNode(s[1],s[3])\n",
    "    term %= cond, lambda h,s: s[1]\n",
    "\n",
    "    factor %= open_parenthesis + arith_expr + closed_parenthesis, lambda h,s: s[2]\n",
    "    factor %= string, lambda h,s: ast_nodes.StringNode(s[1])\n",
    "    factor %= number, lambda h,s: ast_nodes.NumberNode(s[1])\n",
    "    factor %= true, lambda h,s: ast_nodes.BooleanNode(s[1])\n",
    "    factor %= false, lambda h,s: ast_nodes.BooleanNode(s[1])\n",
    "    factor %= var, lambda h,s: ast_nodes.VariableNode(s[1])\n",
    "    factor %= funct, lambda h,s: s[1]\n",
    "    factor %= listnt, lambda h,s: s[1]\n",
    "    factor %= listindexed, lambda h,s: s[1]\n",
    "    factor %= funct_name, lambda h,s: ast_nodes.FunctionName(s[1])\n",
    "\n",
    "    listnt %= open_square + arg_list + closed_square, lambda h,s: ast_nodes.ListNode(s[2])\n",
    "    listindexed %= factor + open_square + factor + closed_square, lambda h,s: ast_nodes.IndexListNode(s[1], s[3])\n",
    "\n",
    "    \n",
    "    ifnt %= ift + open_parenthesis + cond +closed_parenthesis + open_bracket + block + closed_bracket + elseblock, lambda h,s: ast_nodes.IfElse(s[3],s[6],s[8])\n",
    "\n",
    "    elseblock %= elset + open_bracket + block + closed_bracket, lambda h,s: s[3]   \n",
    "    elseblock %= Gram.Epsilon, lambda h,s: []\n",
    "\n",
    "    cond %= factor, lambda h,s: s[1] \n",
    "    cond %= nott + cond, lambda h,s: ast_nodes.Not(s[2])\n",
    "    cond %= factor + andt + cond, lambda h,s: ast_nodes.And(s[1],s[3]) \n",
    "    cond %= factor + ort + cond, lambda h,s: ast_nodes.Or(s[1],s[3])  \n",
    "    cond %= factor + lessthan + factor, lambda h,s: ast_nodes.LessThan(s[1],s[3])  \n",
    "    cond %= factor + morethan + factor, lambda h,s: ast_nodes.MoreThan(s[1],s[3])  \n",
    "    cond %= factor + equalequal + factor, lambda h,s: ast_nodes.EqualEqual(s[1],s[3])  \n",
    "\n",
    "    funct %= funct_name + open_parenthesis + arg_list + closed_parenthesis, lambda h,s: ast_nodes.InstanceFunction(s[1],s[3]) \n",
    "    funct %= factor + point + funct_name + open_parenthesis + arg_list + closed_parenthesis, lambda h,s: ast_nodes.InstanceFunction(s[3],s[5], s[1]) \n",
    "\n",
    "\n",
    "    whilent %= whilet + open_parenthesis + cond +closed_parenthesis + open_bracket + block + closed_bracket, lambda h,s: ast_nodes.WhileNode(s[3],s[6])\n",
    "\n",
    "\n",
    "    functnt %= typet + funct_name + open_parenthesis + arg_types_list + closed_parenthesis + open_bracket + block + closed_bracket, lambda h,s: ast_nodes.FuncDeclaration(s[1],s[2],s[4],s[7])\n",
    "\n",
    "    arg_types_list %= type_var, lambda h,s: [s[1]]\n",
    "    arg_types_list %= arg_types_list + colon + type_var, lambda h,s: s[1] + [s[3]]\n",
    "    arg_types_list %= Gram.Epsilon, lambda h,s: []\n",
    "    \n",
    "    type_var %= typet + var, lambda h,s: [s[1], s[2]]\n",
    "    type_var %= entity + var, lambda h,s: [s[1], s[2]]\n",
    "\n",
    "    return Gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "El parser construye las tablas action y goto, a partir, de la gramática, para después pasarle la lista de tokens y ver que se corresponda a lo definido en la gramática y no haya errores sintácticos.\n",
    "\n",
    "Ahora analizaremos como se construyó el parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.utils import ContainerSet\n",
    "from cmp.pycompiler import Item\n",
    "from cmp.automata import State, multiline_formatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First\n",
    "\n",
    "El conjunto first puede ser computado para los terminales, no terminales y producciones de la gramática, los first se inicializan vacíos y se van actualizando siguiendo las reglas vistas en conferencias:\n",
    "* Si X $\\rightarrow$ W1 | W2 | ... | Wn entonces por definición, First(X) = $\\cup_i$ First($W_i$).\n",
    "* Si X $\\rightarrow \\epsilon$ entonces $\\epsilon \\in$ First(X).\n",
    "* Si W = xZ donde x es un terminal, entonces trivialmente First(W) = { x }.\n",
    "* Si W = YZ donde Y es un no-terminal y Z una forma oracional, entonces First(Y) $\\subseteq$ First(W).\n",
    "* Si W = YZ y $\\epsilon \\in$ First(Y) entonces First(Z) $\\subseteq$ First(W).\n",
    "\n",
    "El algoritmo se hizo en dos partes, primeramente se hizo el método compute_local_first, donde se calculará el First(alpha), siendo alpha una forma oracional, después de esto se hace el método compute_first para calcular todos los conjuntos first, donde se van actualizando los conjuntos iniciales según lo que devuelve compute_local_first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_local_first(firsts, alpha):\n",
    "    first_alpha = ContainerSet()\n",
    "    \n",
    "    try:\n",
    "        alpha_is_epsilon = alpha.IsEpsilon\n",
    "    except:\n",
    "        alpha_is_epsilon = False\n",
    "    \n",
    "    # alpha == epsilon ? First(alpha) = { epsilon }\n",
    "    if alpha_is_epsilon:\n",
    "        first_alpha.set_epsilon()\n",
    "\n",
    "    # alpha = X1 ... XN\n",
    "    else:\n",
    "        for item in alpha:\n",
    "            first_symbol = firsts[item]\n",
    "    # First(Xi) subconjunto First(alpha)\n",
    "            first_alpha.update(first_symbol)\n",
    "    # epsilon pertenece a First(X1)...First(Xi) ? First(Xi+1) subconjunto de First(X) y First(alpha)\n",
    "            if not first_symbol.contains_epsilon:\n",
    "                break\n",
    "    # epsilon pertenece a First(X1)...First(XN) ? epsilon pertence a First(X) y al First(alpha)\n",
    "        else:\n",
    "            first_alpha.set_epsilon()\n",
    "\n",
    "    # First(alpha)\n",
    "    return first_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_firsts(G):\n",
    "    firsts = {}\n",
    "    change = True\n",
    "    \n",
    "    # init First(Vt)\n",
    "    for terminal in G.terminals:\n",
    "        firsts[terminal] = ContainerSet(terminal)\n",
    "        \n",
    "    # init First(Vn)\n",
    "    for nonterminal in G.nonTerminals:\n",
    "        firsts[nonterminal] = ContainerSet()\n",
    "    \n",
    "    while change:\n",
    "        change = False\n",
    "        \n",
    "        # P: X -> alpha\n",
    "        for production in G.Productions:\n",
    "            X = production.Left\n",
    "            alpha = production.Right\n",
    "            \n",
    "            # get current First(X)\n",
    "            first_X = firsts[X]\n",
    "                \n",
    "            # init First(alpha)\n",
    "            try:\n",
    "                first_alpha = firsts[alpha]\n",
    "            except:\n",
    "                first_alpha = firsts[alpha] = ContainerSet()\n",
    "            \n",
    "            # CurrentFirst(alpha)???\n",
    "            local_first = compute_local_first(firsts, alpha)\n",
    "            \n",
    "            # update First(X) and First(alpha) from CurrentFirst(alpha)\n",
    "            change |= first_alpha.hard_update(local_first)\n",
    "            change |= first_X.hard_update(local_first)\n",
    "                    \n",
    "    # First(Vt) + First(Vt) + First(RightSides)\n",
    "    return firsts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Reduce\n",
    "\n",
    "La implementación de este parser es básicamente el concepto visto en conferencia puesto en código. \n",
    "\n",
    "Si se reconoce la tupla (el estado y el tipo del token en que está), indexa en la tabla action esta tupla que nos da la próxima operación a hacer (shift o reduce) y el estado al que se mueve en la siguiente iteración.\n",
    "\n",
    "Se crea una clase ShiftReduceParser donde redefinimos el método __call__ a partir de lo antes dicho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ShiftReduceParser:\n",
    "    SHIFT = 'SHIFT'\n",
    "    REDUCE = 'REDUCE'\n",
    "    OK = 'OK'\n",
    "    \n",
    "    def __init__(self, G, verbose=False):\n",
    "        self.G = G\n",
    "        self.verbose = verbose\n",
    "        self.action = {}\n",
    "        self.goto = {}\n",
    "        self._build_parsing_table()\n",
    "        self.error = ''\n",
    "    \n",
    "    def _build_parsing_table(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, w):\n",
    "        stack = [ 0 ]\n",
    "        cursor = 0\n",
    "        output = []\n",
    "        operations = []\n",
    "        \n",
    "        while True:\n",
    "            state = stack[-1]\n",
    "            lookahead = w[cursor]\n",
    "            if self.verbose: print(stack, '<---||--->', w[cursor:])\n",
    "            \n",
    "            try:\n",
    "                action, tag = self.action[(state, self.G[lookahead.token_type])]\n",
    "                if action == self.SHIFT:\n",
    "                    operations.append(self.SHIFT)\n",
    "                    stack.append(tag)\n",
    "                    cursor += 1\n",
    "\n",
    "                elif action == self.REDUCE:\n",
    "                    operations.append(self.REDUCE)\n",
    "                    output.append(tag)\n",
    "                    for _ in tag.Right: stack.pop()\n",
    "                    a = self.goto[stack[-1],tag.Left]\n",
    "                    stack.append(a)\n",
    "\n",
    "\n",
    "                elif action == self.OK:\n",
    "                    return output,operations\n",
    "                else:\n",
    "                    raise NameError\n",
    "            except:\n",
    "                self.error = f'Syntax error at line {lookahead.line}'\n",
    "                return None, None\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clausura LR1\n",
    "\n",
    "Para realizarla primeramente, nos apoyaremos en dos métodos que implementamos el expand y el compress.\n",
    "\n",
    "* expand ( Y $\\to \\alpha$ . X $\\delta$, c) = { X $\\to$ . $\\beta$, b | b $\\in$ First($\\delta$ c) } este devuelve un conjunto de items que sugiere incluir.\n",
    "\n",
    "* compress coge un conjunto de items LR1 y devuelve el mismo conjunto pero los items que tienen el mismo centro están unidos.\n",
    "\n",
    "Conociendo el concepto de clausura LR1 visto en conferencia, básicamente se traduce a hacer expand por los items y después hacerle compress a el conjunto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Compilacion.parse.auxiliar import expand, compress\n",
    "\n",
    "def closure_lr1(items, firsts):\n",
    "    closure = ContainerSet(*items)\n",
    "    \n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        \n",
    "        new_items = ContainerSet()\n",
    "        \n",
    "        for item in closure:\n",
    "            new_items.extend(expand(item, firsts))\n",
    "\n",
    "        changed = closure.update(new_items)\n",
    "        \n",
    "    return compress(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goto LR1\n",
    "\n",
    "Goto(I,X) = CL($\\{$ Y $\\rightarrow$ $\\alpha$ X. $\\beta$, c | Y $\\rightarrow$ $\\alpha$ .X $\\beta$, c $\\in$ I \\})\n",
    "\n",
    "El método setea just_kernel = true para calcular solamente el conjunto de items kernel y no todo el conjunto, sino, se tiene el conjunto first para calcular la clausura lr1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goto_lr1(items, symbol, firsts=None, just_kernel=False):\n",
    "    assert just_kernel or firsts is not None, '`firsts` must be provided if `just_kernel=False`'\n",
    "    items = frozenset(item.NextItem() for item in items if item.NextSymbol == symbol)\n",
    "    return items if just_kernel else closure_lr1(items, firsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autómata LR1\n",
    "\n",
    "Para construir el autómata partimos del estado inicial Program' -> .Program,$, luego se van haciendo trancisiones con los terminales y no terminales, usando como función de trancisión goto_lr1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LR1_automaton(G):\n",
    "    assert len(G.startSymbol.productions) == 1, 'Grammar must be augmented'\n",
    "    \n",
    "    firsts = compute_firsts(G)\n",
    "    firsts[G.EOF] = ContainerSet(G.EOF)\n",
    "    \n",
    "    start_production = G.startSymbol.productions[0]\n",
    "    start_item = Item(start_production, 0, lookaheads=(G.EOF,))\n",
    "    start = frozenset([start_item])\n",
    "    \n",
    "    closure = closure_lr1(start, firsts)\n",
    "    automaton = State(frozenset(closure), True)\n",
    "    \n",
    "    pending = [ start ]\n",
    "    visited = { start: automaton }\n",
    "    \n",
    "    while pending:\n",
    "        current = pending.pop()\n",
    "        current_state = visited[current]\n",
    "        \n",
    "        for symbol in G.terminals + G.nonTerminals:\n",
    "            kernels = goto_lr1(current_state.state, symbol, just_kernel=True)\n",
    "            \n",
    "            if not kernels:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                next_state = visited[kernels]\n",
    "            except KeyError:\n",
    "                pending.append(kernels)\n",
    "                visited[pending[-1]] = next_state = State(frozenset(goto_lr1(current_state.state, symbol, firsts)), True)\n",
    "            \n",
    "            current_state.add_transition(symbol.Name, next_state)\n",
    "    \n",
    "    automaton.set_formatter(multiline_formatter)\n",
    "    return automaton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR1 Canónico\n",
    "\n",
    "Las tablas goto y action se llenan a partir de las reglas vistas en conferencias:\n",
    "\n",
    "* Sea \"$X \\to \\alpha .c \\omega, s$\" un item del estado $I_i$ y $Goto(I_i,c) = I_j$.  \n",
    "Entonces $ACTION[I_i,c] = `S_j`$.\n",
    "\n",
    "* Sea \"$X \\to \\alpha ., s$\" un item del estado $I_i$.  \n",
    "Entonces $ACTION[I_i,s] = `R_k`$ (producción `k` es $X \\to \\alpha$).\n",
    "\n",
    "* Sea $I_i$ el estado que contiene el item \"$S' \\to S., \\$$\" ($S'$ distinguido).  \n",
    "Entonces $ACTION[I_i,\\$] = `OK`$.\n",
    "\n",
    "* Sea \"$X \\to \\alpha .Y \\omega, s$\" item del estado $I_i$ y $Goto(I_i,Y) = I_j$.  \n",
    "Entonces $GOTO[I_i,Y] = j$.\n",
    "\n",
    "El algoritmo es básicamente, un grupo de if else siguiendo las reglas antes mencionadas. El algoritmo se hace en un método llamado _build_parsing_table que está dentro de una clase LR1Parser que hereda de la clase ShiftReduceParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR1Parser(ShiftReduceParser):\n",
    "    \n",
    "    def _build_parsing_table(self):\n",
    "        G = self.G.AugmentedGrammar(True)\n",
    "        \n",
    "        automaton = build_LR1_automaton(G)\n",
    "        for i, node in enumerate(automaton):\n",
    "            if self.verbose: print(i, '\\t', '\\n\\t '.join(str(x) for x in node.state), '\\n')\n",
    "            node.idx = i\n",
    "\n",
    "        for node in automaton:\n",
    "            idx = node.idx\n",
    "            for item in node.state:\n",
    "                if item.IsReduceItem:\n",
    "                    prod = item.production\n",
    "                    if prod.Left == G.startSymbol:\n",
    "                        LR1Parser._register(self.action, (idx, G.EOF), (ShiftReduceParser.OK, None))\n",
    "                    else:\n",
    "                        for lookahead in item.lookaheads:\n",
    "                            LR1Parser._register(self.action, (idx, lookahead), (ShiftReduceParser.REDUCE, prod))\n",
    "                else:\n",
    "                    next_symbol = item.NextSymbol\n",
    "                    if next_symbol.IsTerminal:\n",
    "                        LR1Parser._register(self.action, (idx, next_symbol), (ShiftReduceParser.SHIFT, node[next_symbol.Name][0].idx))\n",
    "                    else:\n",
    "                        LR1Parser._register(self.goto, (idx, next_symbol), node[next_symbol.Name][0].idx)\n",
    "                pass\n",
    "        \n",
    "    @staticmethod\n",
    "    def _register(table, key, value):\n",
    "        assert key not in table or table[key] == value, 'Shift-Reduce or Reduce-Reduce conflict!!!'\n",
    "        table[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis semántico\n",
    "\n",
    "En esta parte se verificarán los posibles errores semánticos que pueda tener el lenguaje, esto se realiza a través de type_collector, type_builder y type_checker. Los recorridos semánticos por el AST se realizan utilizando el visitor que se encuentra en la carpeta cmp, con este como decorador en los métodos visit que tenemos por cada nodo se van visitando todos los nodos del árbol. Cuando se encuentra un error semántico lo devolvemos y paramos de compilar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type_collector\n",
    "\n",
    "Este módulo guardará en un contexto todos los tipos que pertenezcan al lenguaje, además de los métodos predefinidos que tiene el mismo.\n",
    "\n",
    "Aquí dejamos algunos ejemplos de los tipos y funciones predefinidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmp.visitor as visitor\n",
    "from Compilacion.astree.AST_Nodes import ast_nodes as nodes\n",
    "\n",
    "@visitor.when(nodes.ProgramNode)\n",
    "def visit(self, node):\n",
    "\n",
    "    species_type = self.context.create_type('Species')\n",
    "    land_type = self.context.create_type('Land')\n",
    "    society_type = self.context.create_type('Society')\n",
    "    string_type = self.context.create_type('String')\n",
    "    boolean_type = self.context.create_type('Boolean')\n",
    "    number_type = self.context.create_type('Number')\n",
    "\n",
    "        \n",
    "    land_type.define_method('_deleteInfluence', ['entity_1_name', 'characteristic_1_name', 'entity_2_name', 'characteristic_2_name'], [ string_type, string_type, string_type, string_type], boolean_type)\n",
    "    land_type.define_method('_changeCharacteristic', ['name', 'value', 'liminf', 'limsup', 'mutab', 'dist'], [string_type, list_type, number_type, number_type, number_type, number_type], boolean_type)\n",
    "    society_type.define_method('_changeCharacteristic', ['name', 'value', 'liminf', 'limsup', 'mutab', 'dist'], [string_type, list_type, number_type, number_type, number_type, number_type], boolean_type)\n",
    "    species_type.define_method('_changeCharacteristic', ['name', 'value', 'liminf', 'limsup', 'mutab', 'dist'], [string_type, list_type, number_type, number_type, number_type, number_type], boolean_type)\n",
    "    land_type.define_method('_deleteCharacteristic', ['name'], [string_type], boolean_type)\n",
    "    society_type.define_method('_deleteCharacteristic', ['name'], [string_type], boolean_type)\n",
    "    species_type.define_method('_deleteCharacteristic', ['name'], [string_type], boolean_type)\n",
    "\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type_builder\n",
    "\n",
    "En este recorrido tomaremos todas las declaraciones de variables, las declaraciones de entidades y las declaraciones de funciones, que se irán guardando en el contexto. \n",
    "\n",
    "Dejamos de ejemplo el recorrido al nodo DeclarationEntity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmp.visitor as visitor\n",
    "from Compilacion.astree.AST_Nodes import ast_nodes as nodes\n",
    "from cmp.semantic import SemanticError\n",
    "\n",
    "@visitor.when(nodes.DeclarationEntity)\n",
    "def visit(self,node):\n",
    "    if(self.error):\n",
    "        return\n",
    "    try:\n",
    "        attrType = self.context.get_type(node.entity)\n",
    "        self.current_type.define_attribute(node.var, attrType)\n",
    "    except SemanticError as se:\n",
    "        self.errors.append(se.text)\n",
    "        self.error = True\n",
    "        return se\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type_checker\n",
    "\n",
    "En esta parte se verifica que las variables, funciones y entidades instanciadas ya se encuentren declaradas en su scope o en alguno superior, también se verifica que los tipos sean correctos, o sea, que estén bien declarados o instanciados, por ejemplo, Boolean a = 5 da error semántico, pues el tipo declarado no coincide con el asignado, este y otros errores son los que revisa el type_checker. \n",
    "\n",
    "El siguiente ejemplo muestra el chequeo del nodo AsignationVar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmp.visitor as visitor\n",
    "from Compilacion.astree.AST_Nodes import ast_nodes as nodes\n",
    "from cmp.semantic import SemanticError, TypeCompatible\n",
    "\n",
    "\n",
    "WRONG_SIGNATURE = 'Method \"%s\" already defined in \"%s\" with a different signature.'\n",
    "SELF_IS_READONLY = 'Variable \"self\" is read-only.'\n",
    "LOCAL_ALREADY_DEFINED = 'Variable \"%s\" is already defined in method \"%s\".'\n",
    "INCOMPATIBLE_TYPES = 'Cannot convert \"%s\" into \"%s\".'\n",
    "VARIABLE_NOT_DEFINED = 'Variable \"%s\" is not defined in \"%s\".'\n",
    "INVALID_OPERATION = 'Operation \"%s\" is not defined between \"%s\" and \"%s\".'\n",
    "INVALID_RETURN = 'Return value is not an asignation\".'\n",
    "INVALID_PARAMS = 'Invalid params'\n",
    "INVALID_NAME = \"Inavlid name %s\"\n",
    "\n",
    "@visitor.when(nodes.AsignationVar)\n",
    "def visit(self, node, scope):\n",
    "    if(self.error):\n",
    "        return\n",
    "    var = scope.find_variable(node.var)\n",
    "\n",
    "    type_expr = self.visit(node.arith_expr, scope.create_child())\n",
    "    if(self.error):\n",
    "        return\n",
    "\n",
    "    if var is None:\n",
    "        self.errors.append(VARIABLE_NOT_DEFINED % (node.var, self.current_method.name))\n",
    "        self.error = True\n",
    "        return TypeCompatible()\n",
    "\n",
    "    elif not type_expr.conforms_to(var.type):\n",
    "        self.errors.append(INCOMPATIBLE_TYPES % (type_expr.name, var.type.name))\n",
    "        self.error = True\n",
    "        return TypeCompatible()\n",
    "            \n",
    "\n",
    "    return type_expr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpilado\n",
    "\n",
    "Para transpilar nuestro lenguaje a python hacemos otro recorrido por el AST, en este caso iremos formando un string, donde cada nodo del AST aportará un pedacito a este, cada nodo sabe como transpilarse a python (sabe convertirse en un string que será el código en python de ese nodo), por tanto, al finalizar este recorrido tendremos en un string todo el código que nos entran pero transfomado a python.\n",
    "\n",
    "Aquí dejamos un ejemplo de cómo se transpila a python, mostramos el nodo de declarar una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@visitor.when(nodes.DeclarationVar)\n",
    "def visit(self, node, scope, ident):\n",
    "    self.declared_var.append(node.var)\n",
    "\n",
    "    var_type = self.context.get_type(node.type)\n",
    "\n",
    "    decl = \"z\" + node.var + \" = \" + self.visit(node.arith_expr, scope.create_child(), ident+1)\n",
    "    \n",
    "    scope.define_variable(node.var, var_type)\n",
    "\n",
    "    return decl"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
